{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7a87e8-4e6a-4d4f-bf3b-72f435d106d8",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9f15d5-2b1f-4fd2-b4da-775096d477bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import boto3\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610eb445-cbdb-4a63-a0aa-89ada508666e",
   "metadata": {},
   "source": [
    "## Download images from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fce4b5e-cbdf-4e28-8dee-9fa52e5ccb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_Bucket_Name = 'hurricaneimagebucket'\n",
    "Image_Directory = '../Images'\n",
    "Data_Directory = '../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a74ad2-b4cc-4ac0-99c9-2e53d6a6108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# S3 Bucket Download Function for Images and Data with Pagination\n",
    "def S3_Download(S3_Bucket_Name, Image_Directory, Data_Directory):\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        # List objects with pagination\n",
    "        if continuation_token:\n",
    "            S3_Response = s3_client.list_objects_v2(Bucket=S3_Bucket_Name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            S3_Response = s3_client.list_objects_v2(Bucket=S3_Bucket_Name)\n",
    "\n",
    "        if 'Contents' in S3_Response:\n",
    "            for S3_Object in S3_Response['Contents']:\n",
    "                file_name = S3_Object['Key']\n",
    "\n",
    "                if file_name.endswith('.png'):  # Image file\n",
    "                    file_path = os.path.join(Image_Directory, file_name)\n",
    "                elif file_name.endswith('.json'):  # Data file\n",
    "                    file_path = os.path.join(Data_Directory, file_name)\n",
    "                else:\n",
    "                    continue  \n",
    "\n",
    "                # Create to user path\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "                # Download file to user path\n",
    "                s3_client.download_file(S3_Bucket_Name, file_name, file_path)\n",
    "\n",
    "        # Check if there's more data to retrieve\n",
    "        if S3_Response.get('IsTruncated'): \n",
    "            continuation_token = S3_Response['NextContinuationToken']\n",
    "        else:\n",
    "            break  \n",
    "\n",
    "#S3_Download(S3_Bucket_Name, Image_Directory, Data_Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35bcff3-324a-4d1e-839b-be99a2678e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in ../Images: 2438\n",
      "Number of files in ../Data: 2438\n"
     ]
    }
   ],
   "source": [
    "image_file_count = sum(len(files) for _, _, files in os.walk(Image_Directory))\n",
    "data_file_count = sum(len(files) for _, _, files in os.walk(Data_Directory))\n",
    "\n",
    "print(f\"Number of files in {Image_Directory}: {image_file_count}\")\n",
    "print(f\"Number of files in {Data_Directory}: {data_file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3f9db-e7bb-448c-8057-a1e4da7852a0",
   "metadata": {},
   "source": [
    "The xBD dataset provides annotated high-resolution satellite imagery for assessing building damage, consisting of JSON files and image files. This project focuses on analyzing pre- and post-disaster imagery related to hurricanes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2311c8-bb1d-4ef7-a4b8-a8c012b6fd1a",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9a56de-d673-4536-8390-3ff17f73b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read JSON files\n",
    "def load_JSON_data(Data_Directory):\n",
    "    pre_data = []\n",
    "    post_data = []\n",
    "\n",
    "    for filename in os.listdir(Data_Directory):\n",
    "        file_path = os.path.join(Data_Directory, filename)\n",
    "        \n",
    "        # Ensure the path is a file, not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = json.load(file)\n",
    "\n",
    "            # Filter by hurricane natural disaster only\n",
    "            disaster = content['metadata'].get('disaster')\n",
    "            if disaster and \"hurricane\" in disaster.lower():\n",
    "                img_name = content['metadata'].get('img_name', \"\")\n",
    "                data = {\n",
    "                    'img_name': img_name,\n",
    "                    'lng_lat': content['features'].get('lng_lat'),\n",
    "                    'xy': content['features'].get('xy'),\n",
    "                    'sensor': content['metadata'].get('sensor'),\n",
    "                    'provider_asset_type': content['metadata'].get('provider_asset_type'),\n",
    "                    'gsd': content['metadata'].get('gsd'),\n",
    "                    'capture_date': content['metadata'].get('capture_date'),\n",
    "                    'off_nadir_angle': content['metadata'].get('off_nadir_angle'),\n",
    "                    'pan_resolution': content['metadata'].get('pan_resolution'),\n",
    "                    'sun_azimuth': content['metadata'].get('sun_azimuth'),\n",
    "                    'sun_elevation': content['metadata'].get('sun_elevation'),\n",
    "                    'target_azimuth': content['metadata'].get('target_azimuth'),\n",
    "                    'disaster': disaster,\n",
    "                    'disaster_type': content['metadata'].get('disaster_type'),\n",
    "                    'catalog_id': content['metadata'].get('catalog_id'),\n",
    "                    'original_width': content['metadata'].get('original_width'),\n",
    "                    'original_height': content['metadata'].get('original_height'),\n",
    "                    'width': content['metadata'].get('width'),\n",
    "                    'height': content['metadata'].get('height'),\n",
    "                    'id': content['metadata'].get('id')\n",
    "                }\n",
    "            \n",
    "            # Separate pre and post DataFrame\n",
    "            if \"pre\" in img_name.lower():\n",
    "                pre_data.append(data)\n",
    "            elif \"post\" in img_name.lower():\n",
    "                post_data.append(data)\n",
    "    \n",
    "    hurricane_pre_df = pd.DataFrame(pre_data)\n",
    "    hurricane_post_df = pd.DataFrame(post_data)\n",
    "    \n",
    "    return hurricane_pre_df, hurricane_post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39290ce6-9a28-4786-a45e-82bcaab993d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract pre- and post-disaster images\n",
    "def extract_images(image_folder) :\n",
    "    \n",
    "    # List to store pre- and post-hurricane images\n",
    "    pre_images, post_images = [], []\n",
    "    \n",
    "    print(\"Retrieving pre and post disaster images from:\", image_folder)\n",
    "    \n",
    "    for image in glob.iglob(f'{image_folder}/*') :\n",
    "        if image.endswith(\".png\") :\n",
    "            if \"pre\" in image.lower():\n",
    "                pre_images.append(image)\n",
    "            elif \"post\" in image.lower():\n",
    "                post_images.append(image)\n",
    "    \n",
    "    return pre_images, post_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd627eba-b8e4-4af7-bb37-6f3b5c350ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images_dir, json_dir):\n",
    "    hurricane_pre_df, hurricane_post_df = load_JSON_data(json_dir)\n",
    "    pre_hurricane_images, post_hurricane_images = extract_images(images_dir)\n",
    "    print(f\"\\nTotal pre-disaster images: {len(pre_hurricane_images)}\")\n",
    "    print(f\"\\nTotal post-disaster images: {len(post_hurricane_images)}\")\n",
    "\n",
    "    return hurricane_pre_df, hurricane_post_df, pre_hurricane_images, post_hurricane_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaddd6d-088b-41c7-b75a-0c30ff67e2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
