{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f07ff0-cc28-4cc6-bc63-f47f85961cdd",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4d0466-bfd9-407c-8df9-97a10707e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.layers import Activation, MaxPooling2D, Concatenate, UpSampling2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_THREADS = 6\n",
    "semaphore = threading.Semaphore(MAX_THREADS)\n",
    "threads = []\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus :\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d97e2c-5b82-4cc3-afb1-db92bf0a698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get polygon annotations for each building\n",
    "def get_polygon_annotations(feature) :\n",
    "    poly_annotations = {}\n",
    "    \n",
    "    for feat in feature :\n",
    "        # Convert string format to polygon object\n",
    "        feat_shape = wkt.loads(feat['wkt'])\n",
    "        \n",
    "        # Extract coordinates of the polygon \n",
    "        coords = list(mapping(feat_shape)['coordinates'][0])\n",
    "        \n",
    "        # Store unique id and coordinates for each building as a Numpy array\n",
    "        poly_annotations[feat['properties']['uid']] = (np.array(coords, np.int32))\n",
    "        \n",
    "    return poly_annotations\n",
    "\n",
    "# Function to get image dimensions \n",
    "def get_image_dimensions(image_folder, filename) :\n",
    "\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Read and convert the image to a numpy array to get the size\n",
    "    image = io.imread(image_path)\n",
    "    image_arr = np.array(image)\n",
    "    image_size = image_arr.shape\n",
    "    \n",
    "    return image_size\n",
    "\n",
    "# Function to locate buildings using polygon annotations\n",
    "def mask_polygons(size, poly_annotations) :\n",
    "\n",
    "    # Creating black empty mask image \n",
    "    mask_img = np.zeros(size, dtype=np.uint8)\n",
    "    \n",
    "    for points in poly_annotations :\n",
    "        \n",
    "        # Creating empty mask image to hold one polygon \n",
    "        blank_img = np.zeros(size, dtype=np.uint8)\n",
    "        \n",
    "        # Extract list of points to locate the building\n",
    "        poly = poly_annotations[points]\n",
    "        \n",
    "        # Fill the blank image with polygon points \n",
    "        cv2.fillPoly(blank_img, [poly], (1,1,1))\n",
    "        \n",
    "        # Draw the border around the polygon\n",
    "        cv2.polylines(blank_img, [poly], isClosed=True, color=(2, 2, 2), thickness=2)\n",
    "        \n",
    "        # Adding the filled image to the main mask image\n",
    "        mask_img += blank_img\n",
    "        \n",
    "    # Set pixel values greater than 2 to 0 to retain non-overlapping areas\n",
    "    mask_img[mask_img > 2] = 0\n",
    "    \n",
    "    # Convert non-overlapping areas to white to locate buildings\n",
    "    mask_img[mask_img == 1] = 255\n",
    "    mask_img[mask_img == 2] = 127\n",
    "    \n",
    "    return mask_img\n",
    "\n",
    "# Function to resize images to a standard scale\n",
    "def resize_images(image_paths, masked=False, target_size=(256, 256)):\n",
    "    resized_images=[]\n",
    "    \n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        # Resize the image to the target dimensions\n",
    "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        if(not masked):\n",
    "            # Normalize pixel values to the range [0, 1]\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            image = (image > 0).astype(np.uint8)\n",
    "            if image.shape[2] == 3:\n",
    "                # Convert from 3 channels to 1 channel \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                # Expand dimensions to single channel shape\n",
    "                image = np.expand_dims(image, axis=-1)\n",
    "        resized_images.append(image)\n",
    "    \n",
    "    return resized_images\n",
    "\n",
    "def create_masked_image(feature, image_name, images_dir): \n",
    "    with semaphore:\n",
    "        poly_annotations = get_polygon_annotations(feature)\n",
    "        image_size = get_image_dimensions(images_dir, image_name)\n",
    "        mask_image = mask_polygons(image_size, poly_annotations)\n",
    "            \n",
    "        # Save the mask image to the output folder\n",
    "        filename = image_name.split('.')[0]\n",
    "        masked_dirpath = os.path.join(masked_dir, f\"{filename}_mask.png\")\n",
    "        cv2.imwrite(masked_dirpath, mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350b426d-8fed-4d7a-9e0d-dab7e445342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract damage type for each building\n",
    "def extract_damage_type(feature) :\n",
    "    polygon_data = []\n",
    "    \n",
    "    damage_label_encoding = defaultdict(int)\n",
    "    damage_label_encoding['no-damage'] = 0\n",
    "    damage_label_encoding['minor-damage'] = 1\n",
    "    damage_label_encoding['major-damage'] = 2\n",
    "    damage_label_encoding['destroyed'] = 3\n",
    "    \n",
    "    for feat in feature:\n",
    "        # Extract uid and damage type\n",
    "        uid = feat['properties'].get('uid', None) \n",
    "        damage_type = feat['properties'].get('subtype', 'no-damage')\n",
    "        \n",
    "        if uid:\n",
    "            uid += \".png\"\n",
    "            # Extract polygon coordinates\n",
    "            poly_geom = wkt.loads(feat['wkt'])\n",
    "            polygon_points = np.array(list(poly_geom.exterior.coords))\n",
    "            \n",
    "            if polygon_points.size > 0:\n",
    "                polygon_data.append({\"uid\": uid,\n",
    "                                     \"damage_type\": damage_label_encoding[damage_type],\n",
    "                                     \"polygon_points\": polygon_points})\n",
    "        \n",
    "    return polygon_data\n",
    "    \n",
    "def process_post_img(image_dir, image_filename, polygon_pts, scale_pct) :\n",
    "    \n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    # Image dimensions\n",
    "    image = io.imread(image_path)\n",
    "    img_array = np.array(image)\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    # Compute bounding box using X and Y coordinates\n",
    "    xcoords = polygon_pts[:, 0]\n",
    "    ycoords = polygon_pts[:, 1]\n",
    "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
    "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
    "\n",
    "    # Width and height \n",
    "    xdiff = xmax - xmin\n",
    "    ydiff = ymax - ymin\n",
    "\n",
    "    #Extend image by scale percentage\n",
    "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
    "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
    "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
    "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
    "\n",
    "    return img_array[ymin:ymax, xmin:xmax, :]\n",
    "\n",
    "def create_cropped_image(poly_data, image_name, images_dir): \n",
    "    with semaphore:\n",
    "        for data in poly_data:\n",
    "            uid = data['uid']\n",
    "            polygon_pts = data['polygon_points']\n",
    "            processed_img = process_post_img(images_dir, image_name, polygon_pts, 0.8)\n",
    "            output_path = os.path.join(cropped_dir, f\"{uid}\")\n",
    "            cv2.imwrite(output_path, processed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0572b7-8813-4c0b-b5ea-023409ff655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for masked and cropped images\n",
    "masked_dir = os.path.join(os.pardir, \"masks\")\n",
    "cropped_dir = os.path.join(os.pardir, \"cropped_images\")\n",
    "\n",
    "def preprocess_data(hurricane_pre_df, hurricane_post_df, images_dir):\n",
    "    total_buildings = 0\n",
    "    pre_hurricane_mask_images = []\n",
    "    polygon_data = []\n",
    "    threads = []\n",
    "    \n",
    "    def process_pre_and_post(pre_feature, pre_image_name, post_feature, post_image_name):\n",
    "        \"\"\"Handles processing of both pre- and post-hurricane images in a single thread.\"\"\"\n",
    "        create_masked_image(pre_feature, pre_image_name, images_dir)\n",
    "\n",
    "        if post_feature:\n",
    "            nonlocal total_buildings\n",
    "            poly_data = extract_damage_type(post_feature)\n",
    "            create_cropped_image(poly_data, post_image_name, images_dir)\n",
    "            polygon_data.extend(poly_data)\n",
    "            total_buildings += len(post_feature)\n",
    "            \n",
    "    if not os.path.exists(masked_dir) :\n",
    "        os.makedirs(masked_dir)\n",
    "\n",
    "    masked_files_count = len([f for f in os.listdir(masked_dir) if os.path.isfile(os.path.join(masked_dir, f))])\n",
    "    if masked_files_count != len(hurricane_pre_df):\n",
    "\n",
    "        for pre_row, post_row in zip(hurricane_pre_df.itertuples(index=False, name=\"Pandas\"), hurricane_post_df.itertuples(index=False, name=\"Pandas\")):\n",
    "            pre_feature = pre_row.xy\n",
    "            post_feature = post_row.xy\n",
    "            pre_image_name = pre_row.img_name\n",
    "            post_image_name = post_row.img_name\n",
    "    \n",
    "            # Start thread for pre- and post-image processing\n",
    "            t = threading.Thread(target=process_pre_and_post, args=(pre_feature, pre_image_name, post_feature, post_image_name))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        # Ensure all threads are completed\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        print(f\"Total pre-disaster mask images: {len(pre_hurricane_mask_images)}\")\n",
    "        print(f\"\\nTotal buildings processed: {total_buildings}\")\n",
    "        \n",
    "    else:\n",
    "        for index, post_row in hurricane_post_df.iterrows() :\n",
    "            post_feature = post_row['xy']\n",
    "            poly_data = extract_damage_type(post_feature)\n",
    "            polygon_data.extend(poly_data)\n",
    "\n",
    "    pre_hurricane_mask_images = [image for image in glob.iglob(f'{masked_dir}/*') if image.endswith(\".png\")]\n",
    "\n",
    "    pre_resized_images = resize_images(pre_hurricane_mask_images, masked=False)\n",
    "    pre_mask_resized_images = resize_images(pre_hurricane_mask_images, masked=True)\n",
    "\n",
    "    return pre_resized_images, pre_mask_resized_images, polygon_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49345de-806c-4683-b744-3217f4198222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pre_resized_img, pre_mask_resized_img):\n",
    "    # Convert lists to Numpy arrays \n",
    "    X = np.array(pre_resized_img)\n",
    "    y = np.array(pre_mask_resized_img)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    # Split into training and test sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def classification_split_data(df):\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    #valid_df, test_df = train_test_split(valid_df, test_size=0.1, random_state=42)    \n",
    "    print(train_df.shape)\n",
    "    print(valid_df.shape)\n",
    "    #print(test_df.shape)\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ed1bd7-457e-46b5-97f2-d18943ef3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)  # 1-channel output for binary mask\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e44f4e-a664-4871-85ce-fcebae3811f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unet_conv_block(inputs, num_filters) :\n",
    "    \"\"\"Convolution layer with 3x3 filter \n",
    "    followed by BatchNormalization \n",
    "    and ReLU activation\"\"\"\n",
    "    \n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def Unet_encoder_block(inputs, num_filters) :\n",
    "    \n",
    "    x = Unet_conv_block(inputs, num_filters)\n",
    "    # Max pooling with 2x2 filter\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Unet_decoder_block(inputs, num_filters, skip) :\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding=\"same\")(inputs)\n",
    "    \n",
    "    # Check the dimension of upsampled output and skip connection\n",
    "    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n",
    "        skip = UpSampling2D((2, 2))(skip)\n",
    "        \n",
    "    x = Concatenate()([x, skip])\n",
    "    x = Unet_conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_unet_model(input_shape) :\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # Encoders\n",
    "    encoder1 = Unet_encoder_block(inputs, 64)\n",
    "    encoder2 = Unet_encoder_block(encoder1, 128)\n",
    "    encoder3 = Unet_encoder_block(encoder2, 256)\n",
    "    encoder4 = Unet_encoder_block(encoder3, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bridge1 = Unet_conv_block(encoder4, 1024)\n",
    "    \n",
    "    # Decoders\n",
    "    decoder1 = Unet_decoder_block(bridge1, 512, encoder4)\n",
    "    decoder2 = Unet_decoder_block(decoder1, 256, encoder3)\n",
    "    decoder3 = Unet_decoder_block(decoder2, 128, encoder2)\n",
    "    decoder4 = Unet_decoder_block(decoder3, 64, encoder1)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(decoder4)\n",
    "    \n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    \n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb65d8c-0da5-4c32-a158-d883eccaff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_gen():\n",
    "#     # Data generators\n",
    "#     train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "#     val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "#     # Flow data from dataframe\n",
    "#     train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "#                                                         directory=cropped_dir,\n",
    "#                                                         x_col='building_uid',\n",
    "#                                                         y_col='labels',\n",
    "#                                                         target_size=(128, 128),\n",
    "#                                                         batch_size=32,\n",
    "#                                                         seed=123,\n",
    "#                                                         class_mode=\"categorical\")\n",
    "    \n",
    "#     val_generator = val_datagen.flow_from_dataframe(dataframe=valid_df,\n",
    "#                                                     directory=cropped_dir,\n",
    "#                                                     x_col='building_uid',\n",
    "#                                                     y_col='labels',\n",
    "#                                                     target_size=(128, 128),\n",
    "#                                                     batch_size=32,\n",
    "#                                                     shuffle=False,\n",
    "#                                                     seed=123,\n",
    "#                                                     class_mode=\"categorical\")\n",
    "\n",
    "#     return train_generator, val_generator\n",
    "\n",
    "def data_gen(target_size=(128,128),  batch_size=16):\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255.,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "    # Flow data from dataframe\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=cropped_dir,\n",
    "        x_col='building_uid',\n",
    "        y_col='labels',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        seed=123,\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory=cropped_dir,\n",
    "        x_col='building_uid',\n",
    "        y_col='labels',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        seed=123,\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d365827-8dde-4918-b6b4-d95447683162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    pretrained_resnet50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    \n",
    "    for layer in pretrained_resnet50.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Custom CNN layers\n",
    "    # x = Conv2D(32, (5, 5), activation='relu', padding='same')(inputs)\n",
    "    # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # x = Flatten()(x)\n",
    "    x = Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(224, 224, 3))(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    resnet50_model = pretrained_resnet50(inputs)\n",
    "    resnet50_model = Flatten()(resnet50_model)\n",
    "    \n",
    "    concated_layers = Concatenate()([x, resnet50_model])\n",
    "    \n",
    "    concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "    outputs = Dense(4, activation='relu')(concated_layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    # prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    # model = prune_low_magnitude(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def build_resnet50_model(input_shape):\n",
    "#     inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "#     # Load pretrained ResNet50\n",
    "#     pretrained_resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    \n",
    "#     # Freeze earlier layers\n",
    "#     for layer in pretrained_resnet.layers[:-20]:\n",
    "#         layer.trainable = False\n",
    "    \n",
    "#     # Add ResNet branch\n",
    "#     resnet_output = pretrained_resnet(inputs)\n",
    "#     resnet_output = Flatten()(resnet_output)\n",
    "    \n",
    "#     # Custom convolutional layers\n",
    "#     x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "#     x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "#     x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "#     x = Flatten()(x)\n",
    "    \n",
    "#     # Concatenate features\n",
    "#     concated_layers = Concatenate()([x, resnet_output])\n",
    "    \n",
    "#     # Dense layers with regularization and dropout\n",
    "#     concated_layers = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "#     concated_layers = Dropout(0.5)(concated_layers)\n",
    "#     concated_layers = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "#     concated_layers = Dropout(0.5)(concated_layers)\n",
    "#     outputs = Dense(4, activation='softmax')(concated_layers)\n",
    "    \n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bf8f4e-2b93-4dd0-8b3a-b42bcdc9a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_mobilenet_model(input_shape):\n",
    "#     inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "#     pretrained_mobilenet = MobileNet(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "    \n",
    "#     for layer in pretrained_mobilenet.layers:\n",
    "#         layer.trainable = False\n",
    "        \n",
    "#     x = Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "#     x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "    \n",
    "#     x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "    \n",
    "#     x = Flatten()(x)\n",
    "    \n",
    "#     mobilenet_model = pretrained_mobilenet(inputs)\n",
    "#     mobilenet_model = Flatten()(mobilenet_model)\n",
    "    \n",
    "#     concated_layers = Concatenate()([x, mobilenet_model])\n",
    "    \n",
    "#     concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "#     concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "#     concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "#     outputs = Dense(4, activation='softmax')(concated_layers)\n",
    "    \n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "def build_mobilenet_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Load pretrained MobileNet\n",
    "    pretrained_mobilenet = MobileNet(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Unfreeze last 20 layers for fine-tuning\n",
    "    for layer in pretrained_mobilenet.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Custom convolutional layers\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # MobileNet branch\n",
    "    mobilenet_output = pretrained_mobilenet(inputs)\n",
    "    mobilenet_output = Flatten()(mobilenet_output)\n",
    "    \n",
    "    # Concatenate features\n",
    "    concated_layers = Concatenate()([x, mobilenet_output])\n",
    "    \n",
    "    # Dense layers with regularization and dropout\n",
    "    concated_layers = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    concated_layers = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    outputs = Dense(4, activation='softmax')(concated_layers)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cf80f8-6233-463b-87a8-adc002d89b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_model(input_shape, num_classes) :\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Convolutional Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout to reduce overfitting\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cfd302-6242-472c-829b-979055ad2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return 1e-3 * 0.1**(epoch // 10)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "def train_FCN_model(batch_size=16, epochs=50):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        fcn_model = build_fcn_model((256, 256, 3))\n",
    "        # Compile the model\n",
    "        fcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # Training the model \n",
    "    with tf.device('/gpu:0'):\n",
    "        # fcn_model.summmary()\n",
    "        history = fcn_model.fit(X_train, y_train, \n",
    "                            validation_data=(X_valid, y_valid),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size)\n",
    "        \n",
    "        fcn_model.save('model/FCN/FCN_model.keras')\n",
    "\n",
    "def train_Unet_model(batch_size=16, epochs=50):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        unet_model = build_unet_model((256, 256, 3))\n",
    "        # Compile the model\n",
    "        unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        unet_model.summary()\n",
    "        history = unet_model.fit(X_train, y_train, \n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "        unet_model.save('model/Unet/Unet_model.keras')\n",
    "\n",
    "def train_ResNet50_model(train_generator, val_generator, batch_size=64, epochs=50) :\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        resnet50_model = build_resnet50_model((224, 224, 3))\n",
    "        # Compile the model\n",
    "        resnet50_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = resnet50_model.fit(train_generator, \n",
    "                            validation_data=val_generator,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[early_stopping, lr_scheduler])\n",
    "                            # steps_per_epoch=len(train_generator))\n",
    "    \n",
    "        resnet50_model.save('model/Resnet50/Resnet50_model.keras')\n",
    "\n",
    "def train_MobileNet_model(train_generator, val_generator, batch_size=64, epochs=50) :\n",
    "    samples = train_df['building_uid'].count()\n",
    "    steps = np.ceil(samples/batch_size)\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        mobilenet_model = build_mobilenet_model((224, 224, 3))\n",
    "        # Compile the model\n",
    "        mobilenet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):   \n",
    "        # history = mobilenet_model.fit(train_generator, \n",
    "        #                     validation_data=val_generator,\n",
    "        #                     epochs=epochs,\n",
    "        #                     batch_size=batch_size,\n",
    "        #                     callbacks=[early_stopping, lr_scheduler])\n",
    "                            # steps_per_epoch=len(train_generator))\n",
    "        history = mobilenet_model.fit_generator(generator=train_generator,\n",
    "                            steps_per_epoch=steps,\n",
    "                            epochs=epochs,\n",
    "                            workers=4,\n",
    "                            use_multiprocessing=False,\n",
    "                            callbacks=[early_stopping, lr_scheduler],\n",
    "                            verbose=1)\n",
    "\n",
    "        mobilenet_model.save('model/MobileNet/MobileNet_model.keras')\n",
    "\n",
    "def train_CNN_model(train_generator, val_generator, batch_size=16, epochs=50):   \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        CNN_model = build_CNN_model((224, 224, 3), 4)\n",
    "        # Compile the model\n",
    "        CNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = CNN_model.fit(train_generator, \n",
    "                                validation_data=val_generator,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[early_stopping, lr_scheduler])\n",
    "        CNN_model.save('model/simpleCNN/simpleCNN_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7a1d5-e740-4584-98b4-0f555357f6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
