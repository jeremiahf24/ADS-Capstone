{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2ae47e-9fd9-4631-bd79-b95bf6d80cd3",
   "metadata": {},
   "source": [
    "# Satellite Intelligence for Catastrophic Natural Disaster Recovery: Assessing Damage and First Responder Priorities through Geospatial Imagery\n",
    "## Modeling (Data Preprocessing and Model Training)\n",
    "\n",
    "This notebook covers data preprocessing and model training for two subtasks: building localization and damage classification. It can be executed through the Main.ipynb notebook. The models are not uploaded to GitHub due to their large file size.\n",
    "\n",
    "**Group 4:** Jeremiah Fa'atiliga, Ravita Kartawinata, Sowmiya Kanmani Maruthavanan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7da284-13b5-4c22-b7e2-44ceda90407a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Importing Libraries](#importing-libraries)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "    1. [Preprocessing for Building Localization](#preprocessing-for-building-localization)\n",
    "    2. [Preprocessing for Damage Classification](#preprocessing-for-damage-classification)\n",
    "3. [Data Partitioning](#data-partitioning)\n",
    "4. [Building Localization Model Building](#building-localization-model-building)\n",
    "    1. [Fully Convolutional Network (FCN)](#fully-convolutional-network)\n",
    "    2. [U-Net](#U-Net)\n",
    "5. [Image Data Generators for Classification Models](#image-data-generators-for-classification-models)\n",
    "6. [Damage Classification Model Building](#damage-classification-model-building)\n",
    "    1. [Simple CNN](#simple-cnn)\n",
    "    2. [MobileNet](#mobile-net)\n",
    "    3. [ResNet-50](#resnet-50)\n",
    "7. [Model Training](#model-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f07ff0-cc28-4cc6-bc63-f47f85961cdd",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4d0466-bfd9-407c-8df9-97a10707e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import time\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.layers import Activation, MaxPooling2D, Concatenate, UpSampling2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_THREADS = 6\n",
    "semaphore = threading.Semaphore(MAX_THREADS)\n",
    "threads = []\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus :\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339e9bc-023a-42d7-bab3-ae74e26c35ff",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Preprocessing for Building Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d97e2c-5b82-4cc3-afb1-db92bf0a698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get polygon annotations for each building\n",
    "def get_polygon_annotations(feature) :\n",
    "    poly_annotations = {}\n",
    "    \n",
    "    for feat in feature :\n",
    "        # Convert string format to polygon object\n",
    "        feat_shape = wkt.loads(feat['wkt'])\n",
    "        \n",
    "        # Extract coordinates of the polygon \n",
    "        coords = list(mapping(feat_shape)['coordinates'][0])\n",
    "        \n",
    "        # Store unique id and coordinates for each building as a Numpy array\n",
    "        poly_annotations[feat['properties']['uid']] = (np.array(coords, np.int32))\n",
    "        \n",
    "    return poly_annotations\n",
    "\n",
    "# Function to get image dimensions \n",
    "def get_image_dimensions(image_folder, filename) :\n",
    "\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Read and convert the image to a numpy array to get the size\n",
    "    image = io.imread(image_path)\n",
    "    image_arr = np.array(image)\n",
    "    image_size = image_arr.shape\n",
    "    \n",
    "    return image_size\n",
    "\n",
    "# Function to locate buildings using polygon annotations\n",
    "def mask_polygons(size, poly_annotations) :\n",
    "\n",
    "    # Creating black empty mask image \n",
    "    mask_img = np.zeros(size, dtype=np.uint8)\n",
    "    \n",
    "    for points in poly_annotations :\n",
    "        \n",
    "        # Creating empty mask image to hold one polygon \n",
    "        blank_img = np.zeros(size, dtype=np.uint8)\n",
    "        \n",
    "        # Extract list of points to locate the building\n",
    "        poly = poly_annotations[points]\n",
    "        \n",
    "        # Fill the blank image with polygon points \n",
    "        cv2.fillPoly(blank_img, [poly], (1,1,1))\n",
    "        \n",
    "        # Draw the border around the polygon\n",
    "        cv2.polylines(blank_img, [poly], isClosed=True, color=(2, 2, 2), thickness=2)\n",
    "        \n",
    "        # Adding the filled image to the main mask image\n",
    "        mask_img += blank_img\n",
    "        \n",
    "    # Set pixel values greater than 2 to 0 to retain non-overlapping areas\n",
    "    mask_img[mask_img > 2] = 0\n",
    "    \n",
    "    # Convert non-overlapping areas to white to locate buildings\n",
    "    mask_img[mask_img == 1] = 255\n",
    "    mask_img[mask_img == 2] = 127\n",
    "    \n",
    "    return mask_img\n",
    "\n",
    "# Function to resize images to a standard scale\n",
    "def resize_images(image_paths, masked=False, target_size=(256, 256)):\n",
    "    resized_images=[]\n",
    "    \n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        # Resize the image to the target dimensions\n",
    "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        if(not masked):\n",
    "            # Normalize pixel values to the range [0, 1]\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            image = (image > 0).astype(np.uint8)\n",
    "            if image.shape[2] == 3:\n",
    "                # Convert from 3 channels to 1 channel \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                # Expand dimensions to single channel shape\n",
    "                image = np.expand_dims(image, axis=-1)\n",
    "        resized_images.append(image)\n",
    "    \n",
    "    return resized_images\n",
    "\n",
    "def create_masked_image(feature, image_name, images_dir): \n",
    "    with semaphore:\n",
    "        poly_annotations = get_polygon_annotations(feature)\n",
    "        image_size = get_image_dimensions(images_dir, image_name)\n",
    "        mask_image = mask_polygons(image_size, poly_annotations)\n",
    "            \n",
    "        # Save the mask image to the output folder\n",
    "        filename = image_name.split('.')[0]\n",
    "        masked_dirpath = os.path.join(masked_dir, f\"{filename}_mask.png\")\n",
    "        cv2.imwrite(masked_dirpath, mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d4f57-3c61-4c30-b805-f112f9b95786",
   "metadata": {},
   "source": [
    "### Preprocessing for Damage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350b426d-8fed-4d7a-9e0d-dab7e445342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract damage type for each building\n",
    "def extract_damage_type(feature) :\n",
    "    polygon_data = []\n",
    "    \n",
    "    damage_label_encoding = defaultdict(int)\n",
    "    damage_label_encoding['no-damage'] = 0\n",
    "    damage_label_encoding['minor-damage'] = 1\n",
    "    damage_label_encoding['major-damage'] = 2\n",
    "    damage_label_encoding['destroyed'] = 3\n",
    "    \n",
    "    for feat in feature:\n",
    "        # Extract uid and damage type\n",
    "        uid = feat['properties'].get('uid', None) \n",
    "        damage_type = feat['properties'].get('subtype', 'no-damage')\n",
    "        \n",
    "        if uid:\n",
    "            uid += \".png\"\n",
    "            # Extract polygon coordinates\n",
    "            poly_geom = wkt.loads(feat['wkt'])\n",
    "            polygon_points = np.array(list(poly_geom.exterior.coords))\n",
    "            \n",
    "            if polygon_points.size > 0:\n",
    "                polygon_data.append({\"uid\": uid,\n",
    "                                     \"damage_type\": damage_label_encoding[damage_type],\n",
    "                                     \"polygon_points\": polygon_points})\n",
    "        \n",
    "    return polygon_data\n",
    "    \n",
    "def process_post_img(image_dir, image_filename, polygon_pts, scale_pct) :\n",
    "    \n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    # Image dimensions\n",
    "    image = io.imread(image_path)\n",
    "    img_array = np.array(image)\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    # Compute bounding box using X and Y coordinates\n",
    "    xcoords = polygon_pts[:, 0]\n",
    "    ycoords = polygon_pts[:, 1]\n",
    "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
    "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
    "\n",
    "    # Width and height \n",
    "    xdiff = xmax - xmin\n",
    "    ydiff = ymax - ymin\n",
    "\n",
    "    #Extend image by scale percentage\n",
    "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
    "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
    "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
    "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
    "\n",
    "    return img_array[ymin:ymax, xmin:xmax, :]\n",
    "\n",
    "def create_cropped_image(poly_data, image_name, images_dir): \n",
    "    with semaphore:\n",
    "        for data in poly_data:\n",
    "            uid = data['uid']\n",
    "            polygon_pts = data['polygon_points']\n",
    "            processed_img = process_post_img(images_dir, image_name, polygon_pts, 0.8)\n",
    "            output_path = os.path.join(cropped_dir, f\"{uid}\")\n",
    "            cv2.imwrite(output_path, processed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0572b7-8813-4c0b-b5ea-023409ff655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for masked and cropped images\n",
    "masked_dir = os.path.join(os.pardir, \"masks\")\n",
    "cropped_dir = os.path.join(os.pardir, \"cropped_images\")\n",
    "\n",
    "def preprocess_data(hurricane_pre_df, hurricane_post_df, images_dir):\n",
    "    total_buildings = 0\n",
    "    pre_hurricane_mask_images = []\n",
    "    polygon_data = []\n",
    "    threads = []\n",
    "    \n",
    "    def process_pre_and_post(pre_feature, pre_image_name, post_feature, post_image_name):\n",
    "        \"\"\"Handles processing of both pre- and post-hurricane images in a single thread.\"\"\"\n",
    "        create_masked_image(pre_feature, pre_image_name, images_dir)\n",
    "\n",
    "        if post_feature:\n",
    "            nonlocal total_buildings\n",
    "            poly_data = extract_damage_type(post_feature)\n",
    "            create_cropped_image(poly_data, post_image_name, images_dir)\n",
    "            polygon_data.extend(poly_data)\n",
    "            total_buildings += len(post_feature)\n",
    "            \n",
    "    if not os.path.exists(masked_dir) :\n",
    "        os.makedirs(masked_dir)\n",
    "\n",
    "    masked_files_count = len([f for f in os.listdir(masked_dir) if os.path.isfile(os.path.join(masked_dir, f))])\n",
    "    if masked_files_count != len(hurricane_pre_df):\n",
    "\n",
    "        for pre_row, post_row in zip(hurricane_pre_df.itertuples(index=False, name=\"Pandas\"), hurricane_post_df.itertuples(index=False, name=\"Pandas\")):\n",
    "            pre_feature = pre_row.xy\n",
    "            post_feature = post_row.xy\n",
    "            pre_image_name = pre_row.img_name\n",
    "            post_image_name = post_row.img_name\n",
    "    \n",
    "            # Start thread for pre- and post-image processing\n",
    "            t = threading.Thread(target=process_pre_and_post, args=(pre_feature, pre_image_name, post_feature, post_image_name))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        # Ensure all threads are completed\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        print(f\"Total pre-disaster mask images: {len(pre_hurricane_mask_images)}\")\n",
    "        print(f\"\\nTotal buildings processed: {total_buildings}\")\n",
    "        \n",
    "    else:\n",
    "        for index, post_row in hurricane_post_df.iterrows() :\n",
    "            post_feature = post_row['xy']\n",
    "            poly_data = extract_damage_type(post_feature)\n",
    "            polygon_data.extend(poly_data)\n",
    "\n",
    "    pre_hurricane_mask_images = [image for image in glob.iglob(f'{masked_dir}/*') if image.endswith(\".png\")]\n",
    "\n",
    "    pre_resized_images = resize_images(pre_hurricane_mask_images, masked=False)\n",
    "    pre_mask_resized_images = resize_images(pre_hurricane_mask_images, masked=True)\n",
    "\n",
    "    return pre_resized_images, pre_mask_resized_images, polygon_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ea653-8fd1-4fb3-b246-52f1fb8e4aa0",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49345de-806c-4683-b744-3217f4198222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pre_resized_img, pre_mask_resized_img):\n",
    "    # Convert lists to Numpy arrays \n",
    "    X = np.array(pre_resized_img)\n",
    "    y = np.array(pre_mask_resized_img)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    # Split into training, validation and test sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def classification_split_data(df):\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    valid_df, test_df = train_test_split(valid_df, test_size=0.5, random_state=42)    \n",
    "    print(train_df.shape)\n",
    "    print(valid_df.shape)\n",
    "    print(test_df.shape)\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf19cd4-7749-4078-a1b8-604102384152",
   "metadata": {},
   "source": [
    "## Building Localization Model Building\n",
    "\n",
    "### Fully Convolutional Network (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ed1bd7-457e-46b5-97f2-d18943ef3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)  # 1-channel output for binary mask\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"FCN\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8eeb5-3515-4ac6-95e1-49d07ec83d98",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e44f4e-a664-4871-85ce-fcebae3811f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unet_conv_block(inputs, num_filters) :\n",
    "    \"\"\"Convolution layer with 3x3 filter \n",
    "    followed by BatchNormalization \n",
    "    and ReLU activation\"\"\"\n",
    "    \n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def Unet_encoder_block(inputs, num_filters) :\n",
    "    \n",
    "    x = Unet_conv_block(inputs, num_filters)\n",
    "    # Max pooling with 2x2 filter\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Unet_decoder_block(inputs, num_filters, skip) :\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding=\"same\")(inputs)\n",
    "    \n",
    "    # Check the dimension of upsampled output and skip connection\n",
    "    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n",
    "        skip = UpSampling2D((2, 2))(skip)\n",
    "        \n",
    "    x = Concatenate()([x, skip])\n",
    "    x = Unet_conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_unet_model(input_shape) :\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # Encoders\n",
    "    encoder1 = Unet_encoder_block(inputs, 64)\n",
    "    encoder2 = Unet_encoder_block(encoder1, 128)\n",
    "    encoder3 = Unet_encoder_block(encoder2, 256)\n",
    "    encoder4 = Unet_encoder_block(encoder3, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bridge1 = Unet_conv_block(encoder4, 1024)\n",
    "    \n",
    "    # Decoders\n",
    "    decoder1 = Unet_decoder_block(bridge1, 512, encoder4)\n",
    "    decoder2 = Unet_decoder_block(decoder1, 256, encoder3)\n",
    "    decoder3 = Unet_decoder_block(decoder2, 128, encoder2)\n",
    "    decoder4 = Unet_decoder_block(decoder3, 64, encoder1)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(decoder4)\n",
    "    \n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    \n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade9cd1-7334-4132-85b2-85aa3789b231",
   "metadata": {},
   "source": [
    "## Image Data Generators for Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb65d8c-0da5-4c32-a158-d883eccaff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(train_df_, valid_df_, test_df_, target_size=(128,128),  batch_size=16, num_classes=2):\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255.,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "    test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "    class_mode = 'binary' if num_classes == 2 else 'categorical'\n",
    "\n",
    "    # Flow data from dataframe\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df_,\n",
    "        directory=cropped_dir,\n",
    "        x_col='building_uid',\n",
    "        y_col='labels',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        seed=123,\n",
    "        class_mode=class_mode\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df_,\n",
    "        directory=cropped_dir,\n",
    "        x_col='building_uid',\n",
    "        y_col='labels',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        seed=123,\n",
    "        class_mode=class_mode\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df_,\n",
    "        directory=cropped_dir,\n",
    "        x_col='building_uid',\n",
    "        y_col='labels',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        seed=123,\n",
    "        class_mode=class_mode\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4371d94-ef14-466c-ac9c-8e9b5212b28c",
   "metadata": {},
   "source": [
    "## Damage Classification Model Building\n",
    "\n",
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cf80f8-6233-463b-87a8-adc002d89b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_model(input_shape, num_classes) :\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Convolutional Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout to reduce overfitting\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a09d7d-f73a-4d53-9ae7-851b85fecefe",
   "metadata": {},
   "source": [
    "### MobileNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bf8f4e-2b93-4dd0-8b3a-b42bcdc9a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Load pretrained MobileNet\n",
    "    pretrained_mobilenet = MobileNet(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "    \n",
    "    # Unfreeze last 20 layers for fine-tuning\n",
    "    for layer in pretrained_mobilenet.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Custom convolutional layers\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # MobileNet branch\n",
    "    mobilenet_output = pretrained_mobilenet(inputs)\n",
    "    mobilenet_output = Flatten()(mobilenet_output)\n",
    "    \n",
    "    # Concatenate features\n",
    "    concated_layers = Concatenate()([x, mobilenet_output])\n",
    "    \n",
    "    # Dense layers with regularization and dropout\n",
    "    concated_layers = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    concated_layers = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    outputs = Dense(4, activation='softmax')(concated_layers)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name=\"MobileNet\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1a946-82e3-40e0-987d-0073cfe2dde7",
   "metadata": {},
   "source": [
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d365827-8dde-4918-b6b4-d95447683162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50_model(input_shape, num_classes=4):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    pretrained_resnet = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Freeze earlier layers\n",
    "    for layer in pretrained_resnet.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    resnet_output = pretrained_resnet(inputs)\n",
    "    resnet_output = Flatten()(resnet_output)\n",
    "\n",
    "    # Custom convolutional layers\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Concatenate features\n",
    "    concated_layers = Concatenate()([x, resnet_output])\n",
    "\n",
    "    # Dense layers with regularization and dropout\n",
    "    concated_layers = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    concated_layers = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concated_layers)\n",
    "    concated_layers = Dropout(0.5)(concated_layers)\n",
    "    \n",
    "    # Output layer\n",
    "    activation = 'sigmoid' if num_classes == 2 else 'softmax'\n",
    "    #outputs = Dense(1, activation=activation)(concated_layers)\n",
    "    outputs = Dense(4, activation=activation)(concated_layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"ResNet-50\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6a115-ce02-40e7-bb7e-0037ff53aa7d",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cfd302-6242-472c-829b-979055ad2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return 1e-3 * 0.1**(epoch // 10)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "def train_FCN_model(batch_size=16, epochs=50):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        fcn_model = build_fcn_model((256, 256, 3))\n",
    "        # Compile the model\n",
    "        fcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    with tf.device('/gpu:0'):\n",
    "        # Training the model\n",
    "        history = fcn_model.fit(X_train, y_train, \n",
    "                            validation_data=(X_valid, y_valid),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size)\n",
    "        \n",
    "        fcn_model.save('model/FCN/FCN_model.keras')\n",
    "\n",
    "def train_Unet_model(batch_size=16, epochs=50):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        unet_model = build_unet_model((256, 256, 3))\n",
    "        # Compile the model\n",
    "        unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Training the model\n",
    "        history = unet_model.fit(X_train, y_train, \n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "        unet_model.save('model/Unet/Unet_model.keras')\n",
    "\n",
    "def train_CNN_model(train_generator, val_generator, batch_size=64, epochs=50):   \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        CNN_model = build_CNN_model((128, 128, 3), 4)\n",
    "        # Compile the model\n",
    "        CNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = CNN_model.fit(train_generator, \n",
    "                                validation_data=val_generator,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[early_stopping, lr_scheduler])\n",
    "        CNN_model.save('model/simpleCNN/simpleCNN_model.keras')\n",
    "\n",
    "def train_MobileNet_model(train_generator, val_generator, batch_size=64, epochs=50) :\n",
    "    samples = train_df['building_uid'].count()\n",
    "    steps = np.ceil(samples/batch_size)\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        mobilenet_model = build_mobilenet_model((128, 128, 3))\n",
    "        # Compile the model\n",
    "        mobilenet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):   \n",
    "        history = mobilenet_model.fit(train_generator, \n",
    "                            validation_data=val_generator,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "        mobilenet_model.save('model/MobileNet/MobileNet_model.keras')\n",
    "\n",
    "def train_ResNet50_model(train_generator, val_generator, num_classes, batch_size=64, epochs=50) :\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        resnet50_model = build_resnet50_model((128, 128, 3), num_classes)\n",
    "        loss= 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'\n",
    "        \n",
    "        # Compile the model\n",
    "        resnet50_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=loss, metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = resnet50_model.fit(train_generator, \n",
    "                            validation_data=val_generator,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "        resnet50_model.save_weights('model/Resnet50model_stage1_weights.h5')\n",
    "\n",
    "def train_ResNet50_model_stage2(train_generator, val_generator, num_classes, batch_size=64, epochs=50) :\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Instantiate the model\n",
    "        resnet50_model_stage2 = build_resnet50_model((128, 128, 3), num_classes)\n",
    "        \n",
    "        loss= 'binary_crossentropy' if num_classes==2 else 'categorical_crossentropy'\n",
    "        # Load weights from Stage 1\n",
    "        resnet50_model_stage2.load_weights('model/Resnet50model_stage1_weights.h5', by_name=True, skip_mismatch=True)\n",
    "        \n",
    "        # Unfreeze all layers for fine-tuning\n",
    "        for layer in resnet50_model_stage2.layers:\n",
    "            layer.trainable = True\n",
    "        # Compile the model\n",
    "        resnet50_model_stage2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=loss, metrics=['accuracy'])\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = resnet50_model_stage2.fit(train_generator, \n",
    "                            validation_data=val_generator,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "        resnet50_model_stage2.save('model/Resnet50/Resnet50_model_stage2.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
