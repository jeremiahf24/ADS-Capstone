{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f07ff0-cc28-4cc6-bc63-f47f85961cdd",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4d0466-bfd9-407c-8df9-97a10707e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Activation, MaxPooling2D, Concatenate, UpSampling2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d97e2c-5b82-4cc3-afb1-db92bf0a698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get polygon annotations for each building\n",
    "def get_polygon_annotations(feature) :\n",
    "    poly_annotations = {}\n",
    "    \n",
    "    for feat in feature :\n",
    "        # Convert string format to polygon object\n",
    "        feat_shape = wkt.loads(feat['wkt'])\n",
    "        \n",
    "        # Extract coordinates of the polygon \n",
    "        coords = list(mapping(feat_shape)['coordinates'][0])\n",
    "        \n",
    "        # Store unique id and coordinates for each building as a Numpy array\n",
    "        poly_annotations[feat['properties']['uid']] = (np.array(coords, np.int32))\n",
    "        \n",
    "    return poly_annotations\n",
    "\n",
    "# Function to get image dimensions \n",
    "def get_image_dimensions(image_folder, filename) :\n",
    "\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Read and convert the image to a numpy array to get the size\n",
    "    image = io.imread(image_path)\n",
    "    image_arr = np.array(image)\n",
    "    image_size = image_arr.shape\n",
    "    \n",
    "    return image_size\n",
    "\n",
    "# Function to locate buildings using polygon annotations\n",
    "def mask_polygons(size, poly_annotations) :\n",
    "\n",
    "    # Creating black empty mask image \n",
    "    mask_img = np.zeros(size, dtype=np.uint8)\n",
    "    \n",
    "    for points in poly_annotations :\n",
    "        \n",
    "        # Creating empty mask image to hold one polygon \n",
    "        blank_img = np.zeros(size, dtype=np.uint8)\n",
    "        \n",
    "        # Extract list of points to locate the building\n",
    "        poly = poly_annotations[points]\n",
    "        \n",
    "        # Fill the blank image with polygon points \n",
    "        cv2.fillPoly(blank_img, [poly], (1,1,1))\n",
    "        \n",
    "        # Draw the border around the polygon\n",
    "        cv2.polylines(blank_img, [poly], isClosed=True, color=(2, 2, 2), thickness=2)\n",
    "        \n",
    "        # Adding the filled image to the main mask image\n",
    "        mask_img += blank_img\n",
    "        \n",
    "    # Set pixel values greater than 2 to 0 to retain non-overlapping areas\n",
    "    mask_img[mask_img > 2] = 0\n",
    "    \n",
    "    # Convert non-overlapping areas to white to locate buildings\n",
    "    mask_img[mask_img == 1] = 255\n",
    "    mask_img[mask_img == 2] = 127\n",
    "    \n",
    "    return mask_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e811056-eb63-4b69-83e0-c4f881024e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_images(masked_dir, hurricane_pre_df, images_dir):\n",
    "    os.makedirs(masked_dir, exist_ok=True) \n",
    "    #TODO SK: if the length inside masked_dir is == len(images) , then dont run the below code\n",
    "    for index, row in hurricane_pre_df.iterrows() :\n",
    "        feature = row['xy']\n",
    "        poly_annotations = get_polygon_annotations(feature)\n",
    "        image_size = get_image_dimensions(images_dir, row['img_name'])\n",
    "        mask_image = mask_polygons(image_size, poly_annotations)\n",
    "        \n",
    "        # Save the mask image to the output folder\n",
    "        filename = row['img_name'].split('.')[0]\n",
    "        masked_dirpath = os.path.join(masked_dir, f\"{filename}_mask.png\")\n",
    "        cv2.imwrite(masked_dirpath, mask_image)\n",
    "        \n",
    "    print(f\"Completed saving mask images to the {masked_dir} directory\")\n",
    "    \n",
    "    pre_hurricane_mask_images = []\n",
    "    \n",
    "    for image in glob.iglob(f'{masked_dir}/*'):\n",
    "        if image.endswith(\".png\"):\n",
    "            pre_hurricane_mask_images.append(image)\n",
    "    \n",
    "    print(f\"Total pre-disaster mask images: {len(pre_hurricane_mask_images)}\")\n",
    "    return pre_hurricane_mask_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed79194-b52f-4d0d-9f07-7d6e1d8da281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize images to a standard scale\n",
    "def resize_images(image_paths, masked=False, target_size=(256, 256)):\n",
    "    resized_images=[]\n",
    "    \n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path)\n",
    "        # Resize the image to the target dimensions\n",
    "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        if(not masked):\n",
    "            # Normalize pixel values to the range [0, 1]\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            image = (image > 0).astype(np.uint8)\n",
    "            if image.shape[2] == 3:\n",
    "                # Convert from 3 channels to 1 channel \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                # Expand dimensions to single channel shape\n",
    "                image = np.expand_dims(image, axis=-1)\n",
    "        resized_images.append(image)\n",
    "    \n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0572b7-8813-4c0b-b5ea-023409ff655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(hurricane_pre_df, images_dir):\n",
    "    # Output folder to save mask images\n",
    "    masked_dir = os.path.join(os.pardir, \"masks\")\n",
    "    pre_hurricane_mask_images= create_masked_images(masked_dir, hurricane_pre_df, images_dir)\n",
    "    pre_resized_img = resize_images(pre_hurricane_images)\n",
    "    pre_mask_resized_img = resize_images(pre_hurricane_mask_images, masked=True)\n",
    "\n",
    "    return pre_resized_img, pre_mask_resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49345de-806c-4683-b744-3217f4198222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pre_resized_img, pre_mask_resized_img):\n",
    "    # Convert lists to Numpy arrays \n",
    "    X = np.array(pre_resized_img)\n",
    "    y = np.array(pre_mask_resized_img)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    # Split into training and test sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ed1bd7-457e-46b5-97f2-d18943ef3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)  # 1-channel output for binary mask\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "fcn_model = build_fcn_model((256, 256, 3))\n",
    "# Compile the model\n",
    "fcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e44f4e-a664-4871-85ce-fcebae3811f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 512, 32)\n"
     ]
    }
   ],
   "source": [
    "def Unet_conv_block(inputs, num_filters) :\n",
    "    \"\"\"Convolution layer with 3x3 filter \n",
    "    followed by BatchNormalization \n",
    "    and ReLU activation\"\"\"\n",
    "    \n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "    \n",
    "inputs = tf.keras.Input((256, 256, 3))\n",
    "x = Unet_conv_block(inputs, 32)\n",
    "\n",
    "def Unet_encoder_block(inputs, num_filters) :\n",
    "    \n",
    "    x = Unet_conv_block(inputs, num_filters)\n",
    "    # Max pooling with 2x2 filter\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "inputs = tf.keras.Input((256, 256, 3))\n",
    "x = Unet_encoder_block(inputs, 32)\n",
    "\n",
    "def Unet_decoder_block(inputs, num_filters, skip) :\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding=\"same\")(inputs)\n",
    "    \n",
    "    # Check the dimension of upsampled output and skip connection\n",
    "    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n",
    "        skip = UpSampling2D((2, 2))(skip)\n",
    "        \n",
    "    x = Concatenate()([x, skip])\n",
    "    x = Unet_conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "inputs = tf.keras.Input((256, 256, 3))\n",
    "skip = tf.keras.Input((512, 512, 3))\n",
    "x = Unet_decoder_block(inputs, 32, skip)\n",
    "print(x.shape)\n",
    "\n",
    "def build_unet_model(input_shape) :\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # Encoders\n",
    "    encoder1 = Unet_encoder_block(inputs, 64)\n",
    "    encoder2 = Unet_encoder_block(encoder1, 128)\n",
    "    encoder3 = Unet_encoder_block(encoder2, 256)\n",
    "    encoder4 = Unet_encoder_block(encoder3, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bridge1 = Unet_conv_block(encoder4, 1024)\n",
    "    \n",
    "    # Decoders\n",
    "    decoder1 = Unet_decoder_block(bridge1, 512, encoder4)\n",
    "    decoder2 = Unet_decoder_block(decoder1, 256, encoder3)\n",
    "    decoder3 = Unet_decoder_block(decoder2, 128, encoder2)\n",
    "    decoder4 = Unet_decoder_block(decoder3, 64, encoder1)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(decoder4)\n",
    "    \n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    \n",
    "    return unet_model\n",
    "\n",
    "# Instantiate the model\n",
    "unet_model = build_unet_model((256, 256, 3))\n",
    "# Compile the model\n",
    "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cfd302-6242-472c-829b-979055ad2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FCN_model(batch_size=16, epochs=50):\n",
    "    # Training the model    \n",
    "    fcn_model.summmary()\n",
    "    history = fcn_model.fit(X_train, y_train, \n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    fcn_model.save('model/FCN/FCN_model.keras')\n",
    "\n",
    "def train_Unet_model(batch_size=16, epochs=50):\n",
    "    unet_model.summary()\n",
    "    history = unet_model.fit(X_train, y_train, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size)\n",
    "    unet_model.save('model/Unet/Unet_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
